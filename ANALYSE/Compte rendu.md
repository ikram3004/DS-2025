# COMPTE RENDU : HOUSE PRICES DATASET

## TABLE DES MATI√àRES

1. [Introduction](#1-introduction)
2. [Description du Dataset](#2-description-du-dataset)
3. [Localisation G√©ographique](#3-localisation-g√©ographique)
4. [Analyse Exploratoire des Donn√©es](#4-analyse-exploratoire-des-donn√©es)
5. [Visualisations Graphiques](#5-visualisations-graphiques)
6. [Matrice de Corr√©lation](#6-matrice-de-corr√©lation)
7. [Mod√®les de R√©gression](#7-mod√®les-de-r√©gression)
8. [Conclusion](#8-conclusion)

---

## 1. INTRODUCTION

Le **House Prices Dataset** est un jeu de donn√©es provenant d'une comp√©tition Kaggle intitul√©e "House Prices: Advanced Regression Techniques". L'objectif principal est de pr√©dire le prix de vente final des maisons r√©sidentielles bas√© sur 79 variables explicatives.

**Probl√©matique** : Comment pr√©dire avec pr√©cision le prix d'une maison en fonction de ses caract√©ristiques physiques, sa localisation et ses √©quipements ?

URL : https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques 
---

## 2. DESCRIPTION DU DATASET

### Caract√©ristiques g√©n√©rales
- **Nombre d'observations (train)** : 1460 maisons
- **Nombre d'observations (test)** : 1459 maisons
- **Nombre de variables** : 81 (79 features + Id + SalePrice)
- **Variable cible** : SalePrice (prix de vente en dollars)

### Types de variables
- **Variables num√©riques** : 38 (superficie, ann√©e, nombre de pi√®ces, etc.)
- **Variables cat√©gorielles** : 43 (qualit√©, type de maison, quartier, etc.)

### Principales variables
- **OverallQual** : Qualit√© g√©n√©rale de la maison (1-10)
- **GrLivArea** : Surface habitable au-dessus du sol (pieds carr√©s)
- **GarageCars** : Capacit√© du garage en nombre de voitures
- **TotalBsmtSF** : Surface totale du sous-sol (pieds carr√©s)
- **YearBuilt** : Ann√©e de construction
- **Neighborhood** : Quartier o√π se situe la maison

---

## 3. LOCALISATION G√âOGRAPHIQUE

### Lieu r√©el du dataset
**Ville** : Ames, Iowa, √âtats-Unis

**Coordonn√©es g√©ographiques** :
- Latitude : 42.0308¬∞ N
- Longitude : 93.6319¬∞ O

**Contexte g√©ographique** :
- Ames est une ville universitaire de l'Iowa, situ√©e dans le Midwest am√©ricain
- Population : environ 66 000 habitants
- Abrite l'Iowa State University
- √âconomie bas√©e sur l'√©ducation, la recherche et l'agriculture
- March√© immobilier repr√©sentatif des villes moyennes am√©ricaines

**P√©riode de collecte** : Les donn√©es couvrent les ventes de maisons de 2006 √† 2010.

**Caract√©ristiques du march√© local** :
- Prix m√©dian des maisons : environ $180,000
- March√© relativement stable compar√© aux grandes m√©tropoles
- Diversit√© architecturale : maisons de style victorien, ranch, colonial

---

## 4. ANALYSE EXPLORATOIRE DES DONN√âES

### Statistiques descriptives de SalePrice

```
Moyenne        : $180,921
M√©diane        : $163,000
√âcart-type     : $79,442
Minimum        : $34,900
Maximum        : $755,000
```

### Distribution du prix de vente
- Distribution asym√©trique vers la droite (skewness positif)
- Pr√©sence de valeurs extr√™mes (maisons de luxe)
- La plupart des maisons se vendent entre $100,000 et $250,000

### Valeurs manquantes principales
- **PoolQC** : 99.5% manquant (peu de maisons ont une piscine)
- **MiscFeature** : 96.3% manquant
- **Alley** : 93.8% manquant
- **Fence** : 80.8% manquant
- **FireplaceQu** : 47.3% manquant

---

## 5. VISUALISATIONS GRAPHIQUES

### Code Python pour les graphiques

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Chargement des donn√©es
train = pd.read_csv('train.csv')

# Configuration style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)

# 1. DISTRIBUTION DU PRIX DE VENTE
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.histplot(train['SalePrice'], kde=True, bins=50, color='skyblue')
plt.title('Distribution du Prix de Vente', fontsize=14, fontweight='bold')
plt.xlabel('Prix de Vente ($)')
plt.ylabel('Fr√©quence')

plt.subplot(1, 2, 2)
stats.probplot(train['SalePrice'], dist="norm", plot=plt)
plt.title('Q-Q Plot du Prix de Vente', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# 2. TOP 10 DES VARIABLES CORREL√âES AVEC SALEPRICE
correlations = train.corr()['SalePrice'].sort_values(ascending=False)
top_features = correlations[1:11]

plt.figure(figsize=(10, 6))
sns.barplot(x=top_features.values, y=top_features.index, palette='viridis')
plt.title('Top 10 des Variables Corr√©l√©es avec SalePrice', fontsize=14, fontweight='bold')
plt.xlabel('Coefficient de Corr√©lation')
plt.tight_layout()
plt.show()

# 3. RELATION ENTRE SURFACE HABITABLE ET PRIX
plt.figure(figsize=(10, 6))
sns.scatterplot(data=train, x='GrLivArea', y='SalePrice', alpha=0.6, color='coral')
plt.title('Prix de Vente vs Surface Habitable', fontsize=14, fontweight='bold')
plt.xlabel('Surface Habitable (pieds carr√©s)')
plt.ylabel('Prix de Vente ($)')
plt.tight_layout()
plt.show()

# 4. PRIX PAR QUALIT√â G√âN√âRALE
plt.figure(figsize=(12, 6))
sns.boxplot(data=train, x='OverallQual', y='SalePrice', palette='Set2')
plt.title('Prix de Vente par Qualit√© G√©n√©rale', fontsize=14, fontweight='bold')
plt.xlabel('Qualit√© G√©n√©rale (1-10)')
plt.ylabel('Prix de Vente ($)')
plt.tight_layout()
plt.show()

# 5. PRIX PAR QUARTIER (TOP 10)
neighborhood_prices = train.groupby('Neighborhood')['SalePrice'].median().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 6))
sns.barplot(x=neighborhood_prices.values, y=neighborhood_prices.index, palette='coolwarm')
plt.title('Top 10 Quartiers par Prix M√©dian', fontsize=14, fontweight='bold')
plt.xlabel('Prix M√©dian ($)')
plt.ylabel('Quartier')
plt.tight_layout()
plt.show()

# 6. √âVOLUTION DES PRIX PAR ANN√âE DE CONSTRUCTION
plt.figure(figsize=(12, 6))
year_prices = train.groupby('YearBuilt')['SalePrice'].mean()
plt.plot(year_prices.index, year_prices.values, linewidth=2, color='darkblue')
plt.title('Prix Moyen par Ann√©e de Construction', fontsize=14, fontweight='bold')
plt.xlabel('Ann√©e de Construction')
plt.ylabel('Prix Moyen ($)')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## 6. MATRICE DE CORR√âLATION

### Code Python pour la matrice de corr√©lation

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Chargement des donn√©es
train = pd.read_csv('train.csv')

# S√©lection des variables num√©riques
numerical_features = train.select_dtypes(include=[np.number]).columns.tolist()

# 1. MATRICE DE CORR√âLATION COMPL√àTE
plt.figure(figsize=(16, 14))
correlation_matrix = train[numerical_features].corr()
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
            annot=False)
plt.title('Matrice de Corr√©lation - Toutes Variables Num√©riques', 
          fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# 2. MATRICE DE CORR√âLATION - TOP 15 VARIABLES
top_corr_features = correlation_matrix['SalePrice'].abs().sort_values(ascending=False).head(16).index

plt.figure(figsize=(12, 10))
sns.heatmap(train[top_corr_features].corr(), annot=True, fmt='.2f', 
            cmap='RdYlGn', center=0, square=True, linewidths=1,
            cbar_kws={"shrink": 0.8})
plt.title('Matrice de Corr√©lation - Top 15 Variables', 
          fontsize=14, fontweight='bold', pad=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# 3. ANALYSE DES CORR√âLATIONS AVEC SALEPRICE
print("=" * 60)
print("CORR√âLATIONS AVEC SALEPRICE")
print("=" * 60)
correlations_with_price = correlation_matrix['SalePrice'].sort_values(ascending=False)

print("\nüîº TOP 10 CORR√âLATIONS POSITIVES :")
print(correlations_with_price.head(11))

print("\nüîΩ TOP 10 CORR√âLATIONS N√âGATIVES :")
print(correlations_with_price.tail(10))

# 4. D√âTECTION DE MULTICOLIN√âARIT√â
print("\n" + "=" * 60)
print("D√âTECTION DE MULTICOLIN√âARIT√â")
print("=" * 60)

# Paires de variables avec corr√©lation > 0.8 (hors diagonale)
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.8:
            high_corr_pairs.append({
                'Variable 1': correlation_matrix.columns[i],
                'Variable 2': correlation_matrix.columns[j],
                'Corr√©lation': correlation_matrix.iloc[i, j]
            })

if high_corr_pairs:
    multicolinearity_df = pd.DataFrame(high_corr_pairs)
    print("\n‚ö†Ô∏è Paires de variables fortement corr√©l√©es (|r| > 0.8) :")
    print(multicolinearity_df.to_string(index=False))
else:
    print("\n‚úì Aucune multicolin√©arit√© forte d√©tect√©e")
```

### Principales corr√©lations observ√©es

**Corr√©lations positives fortes avec SalePrice** :
1. OverallQual (0.79) - Qualit√© g√©n√©rale
2. GrLivArea (0.71) - Surface habitable
3. GarageCars (0.64) - Capacit√© garage
4. GarageArea (0.62) - Surface garage
5. TotalBsmtSF (0.61) - Surface sous-sol

**Corr√©lations n√©gatives** :
- Les corr√©lations n√©gatives sont g√©n√©ralement faibles
- Aucune variable ne montre une corr√©lation n√©gative forte

---

## 7. MOD√àLES DE R√âGRESSION

### Code Python pour les mod√®les

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns

# ========================================
# 1. PR√âPARATION DES DONN√âES
# ========================================

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# S√©paration features et target
X = train.drop(['SalePrice', 'Id'], axis=1)
y = train['SalePrice']
test_ids = test['Id']
X_test = test.drop('Id', axis=1)

# Gestion des valeurs manquantes
# Pour les variables num√©riques : remplir avec la m√©diane
numeric_features = X.select_dtypes(include=[np.number]).columns
for col in numeric_features:
    X[col].fillna(X[col].median(), inplace=True)
    X_test[col].fillna(X_test[col].median(), inplace=True)

# Pour les variables cat√©gorielles : remplir avec 'None'
categorical_features = X.select_dtypes(include=['object']).columns
for col in categorical_features:
    X[col].fillna('None', inplace=True)
    X_test[col].fillna('None', inplace=True)

# Encodage des variables cat√©gorielles
label_encoders = {}
for col in categorical_features:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    X_test[col] = le.transform(X_test[col].astype(str))
    label_encoders[col] = le

# Split train/validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("‚úì Pr√©paration des donn√©es termin√©e")
print(f"  - Forme X_train: {X_train.shape}")
print(f"  - Forme X_val: {X_val.shape}")
print(f"  - Forme y_train: {y_train.shape}")

# ========================================
# 2. ENTRA√éNEMENT DES MOD√àLES
# ========================================

models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=10.0),
    'Lasso': Lasso(alpha=100.0),
    'ElasticNet': ElasticNet(alpha=100.0, l1_ratio=0.5),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
}

results = []

print("\n" + "="*80)
print("ENTRA√éNEMENT ET √âVALUATION DES MOD√àLES")
print("="*80)

for name, model in models.items():
    print(f"\nüìä Entra√Ænement : {name}...")
    
    # Entra√Ænement
    if name in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_val_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    
    # M√©triques
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    mae = mean_absolute_error(y_val, y_pred)
    r2 = r2_score(y_val, y_pred)
    
    results.append({
        'Mod√®le': name,
        'RMSE': rmse,
        'MAE': mae,
        'R¬≤ Score': r2
    })
    
    print(f"  ‚úì RMSE: ${rmse:,.2f}")
    print(f"  ‚úì MAE: ${mae:,.2f}")
    print(f"  ‚úì R¬≤ Score: {r2:.4f}")

# ========================================
# 3. COMPARAISON DES R√âSULTATS
# ========================================

results_df = pd.DataFrame(results).sort_values('RMSE')

print("\n" + "="*80)
print("TABLEAU R√âCAPITULATIF DES PERFORMANCES")
print("="*80)
print(results_df.to_string(index=False))

# Visualisation des performances
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# RMSE
axes[0].barh(results_df['Mod√®le'], results_df['RMSE'], color='coral')
axes[0].set_xlabel('RMSE ($)')
axes[0].set_title('Root Mean Squared Error', fontweight='bold')
axes[0].invert_yaxis()

# MAE
axes[1].barh(results_df['Mod√®le'], results_df['MAE'], color='skyblue')
axes[1].set_xlabel('MAE ($)')
axes[1].set_title('Mean Absolute Error', fontweight='bold')
axes[1].invert_yaxis()

# R¬≤ Score
axes[2].barh(results_df['Mod√®le'], results_df['R¬≤ Score'], color='lightgreen')
axes[2].set_xlabel('R¬≤ Score')
axes[2].set_title('Coefficient de D√©termination', fontweight='bold')
axes[2].invert_yaxis()
axes[2].set_xlim([0, 1])

plt.tight_layout()
plt.show()

# ========================================
# 4. PR√âDICTIONS AVEC LE MEILLEUR MOD√àLE
# ========================================

best_model_name = results_df.iloc[0]['Mod√®le']
best_model = models[best_model_name]

print(f"\nüèÜ Meilleur mod√®le : {best_model_name}")

# Graphique : Valeurs r√©elles vs pr√©dites
if best_model_name in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet']:
    y_pred_best = best_model.predict(X_val_scaled)
else:
    y_pred_best = best_model.predict(X_val)

plt.figure(figsize=(10, 6))
plt.scatter(y_val, y_pred_best, alpha=0.6, color='purple')
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('Prix R√©els ($)')
plt.ylabel('Prix Pr√©dits ($)')
plt.title(f'Pr√©dictions vs R√©alit√© - {best_model_name}', fontweight='bold', fontsize=14)
plt.tight_layout()
plt.show()

# Distribution des r√©sidus
residuals = y_val - y_pred_best

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(residuals, kde=True, bins=50, color='teal')
plt.xlabel('R√©sidus ($)')
plt.ylabel('Fr√©quence')
plt.title('Distribution des R√©sidus', fontweight='bold')

plt.subplot(1, 2, 2)
plt.scatter(y_pred_best, residuals, alpha=0.6, color='orange')
plt.axhline(y=0, color='r', linestyle='--', lw=2)
plt.xlabel('Prix Pr√©dits ($)')
plt.ylabel('R√©sidus ($)')
plt.title('R√©sidus vs Pr√©dictions', fontweight='bold')

plt.tight_layout()
plt.show()

print("\n‚úì Analyse de r√©gression termin√©e")
```

### R√©sultats attendus

Les mod√®les bas√©s sur les ensembles (Random Forest, Gradient Boosting, XGBoost) obtiennent g√©n√©ralement les meilleures performances avec :
- **RMSE** : entre $25,000 et $35,000
- **R¬≤ Score** : entre 0.85 et 0.92
- **MAE** : entre $15,000 et $20,000

---

## 8. CONCLUSION

### Points cl√©s
‚úÖ **Dataset riche** : 79 variables permettent une mod√©lisation d√©taill√©e
‚úÖ **Corr√©lations identifi√©es** : La qualit√© g√©n√©rale et la surface habitable sont les pr√©dicteurs les plus forts
‚úÖ **Mod√®les performants** : Les algorithmes d'ensemble (XGBoost, Random Forest) surpassent les mod√®les lin√©aires
‚úÖ **Localisation r√©elle** : Donn√©es provenant d'Ames, Iowa (2006-2010)

### Recommandations
1. **Feature Engineering** : Cr√©er des variables d√©riv√©es (surface totale, √¢ge de la maison)
2. **Traitement des outliers** : Supprimer les valeurs extr√™mes pour am√©liorer les pr√©dictions
3. **Optimisation des hyperparam√®tres** : Utiliser GridSearchCV ou RandomizedSearchCV
4. **Transformation de la variable cible** : Appliquer log(SalePrice) pour normaliser la distribution
5. **Validation crois√©e** : Utiliser K-Fold pour une √©valuation plus robuste

### Applications pratiques
- Estimation automatique de prix pour agences immobili√®res
- Aide √† la d√©cision pour acheteurs et vendeurs
- Analyse de march√© immobilier
- D√©tection de bonnes affaires (maisons sous-√©valu√©es)

---

**Auteur** : Analyse r√©alis√©e dans le cadre de l'√©tude du dataset House Prices  
**Date** : Novembre 2025  
**Source** : Kaggle - House Prices: Advanced Regression Techniques
